{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation test with ALBEF for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from models.model_vqa import ALBEF\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "\n",
    "import utils\n",
    "from dataset.utils import save_result\n",
    "from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn\n",
    "\n",
    "from scheduler import create_scheduler\n",
    "from optim import create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# prep downloaded data\n",
    "VQA_DATA_DIR = 'data/vqa'\n",
    "\n",
    "zip_files = [\n",
    "    'train2014.zip',\n",
    "    'test2015.zip',\n",
    "    'v2_Annotations_Train_mscoco.zip',\n",
    "    'v2_Annotations_Val_mscoco.zip',\n",
    "    'v2_Complementary_Pairs_Train_mscoco.zip',\n",
    "    'v2_Complementary_Pairs_Val_mscoco.zip',\n",
    "    'v2_Questions_Test_mscoco.zip',\n",
    "    'v2_Questions_Train_mscoco.zip',\n",
    "    'v2_Questions_Val_mscoco.zip',\n",
    "    'val2014.zip'\n",
    "]\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    zip_path = os.path.join(VQA_DATA_DIR, zip_file)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(VQA_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up VQA config\n",
    "args = argparse.Namespace()\n",
    "args.config = './configs/VQA.yaml'\n",
    "args.checkpoint = './ALBEF_4M.pth'\n",
    "args.output_dir = './output/vqa'\n",
    "args.evaluate = False\n",
    "args.text_encoder = 'bert-base-uncased'\n",
    "args.text_decoder = 'bert-base-uncased'\n",
    "args.device = 'cuda'\n",
    "args.seed = 42\n",
    "args.distributed = False\n",
    "\n",
    "config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.result_dir = os.path.join(args.output_dir, 'result')\n",
    "\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n"
     ]
    }
   ],
   "source": [
    "utils.init_distributed_mode(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "start_epoch = 0\n",
    "max_epoch = config['schedular']['epochs']\n",
    "warmup_steps = config['schedular']['warmup_epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vqa datasets\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/vqa_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#### Dataset ####\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating vqa datasets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m datasets = \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvqa\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.distributed:\n\u001b[32m      6\u001b[39m     num_tasks = utils.get_world_size()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/2025Spring/ALBEF/dataset/__init__.py:51\u001b[39m, in \u001b[36mcreate_dataset\u001b[39m\u001b[34m(dataset, config)\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, val_dataset, test_dataset   \n\u001b[32m     50\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataset==\u001b[33m'\u001b[39m\u001b[33mvqa\u001b[39m\u001b[33m'\u001b[39m: \n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     train_dataset = \u001b[43mvqa_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_file\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvqa_root\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvg_root\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     52\u001b[39m     vqa_test_dataset = vqa_dataset(config[\u001b[33m'\u001b[39m\u001b[33mtest_file\u001b[39m\u001b[33m'\u001b[39m], test_transform, config[\u001b[33m'\u001b[39m\u001b[33mvqa_root\u001b[39m\u001b[33m'\u001b[39m], config[\u001b[33m'\u001b[39m\u001b[33mvg_root\u001b[39m\u001b[33m'\u001b[39m], split=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m, answer_list=config[\u001b[33m'\u001b[39m\u001b[33manswer_list\u001b[39m\u001b[33m'\u001b[39m])       \n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, vqa_test_dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/2025Spring/ALBEF/dataset/vqa_dataset.py:14\u001b[39m, in \u001b[36mvqa_dataset.__init__\u001b[39m\u001b[34m(self, ann_file, transform, vqa_root, vg_root, eos, split, max_ques_words, answer_list)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.ann = []\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m ann_file:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28mself\u001b[39m.ann += json.load(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.transform = transform\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.vqa_root = vqa_root\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/vqa_train.json'"
     ]
    }
   ],
   "source": [
    "#### Dataset ####\n",
    "print(\"Creating vqa datasets\")\n",
    "datasets = create_dataset('vqa', config)\n",
    "\n",
    "if args.distributed:\n",
    "    num_tasks = utils.get_world_size()\n",
    "    global_rank = utils.get_rank()\n",
    "    samplers = create_sampler(datasets, [True, False], num_tasks, global_rank)\n",
    "else:\n",
    "    samplers = [None, None]\n",
    "\n",
    "train_loader, test_loader = create_loader(datasets,samplers,\n",
    "                                          batch_size=[config['batch_size_train'],config['batch_size_test']],\n",
    "                                          num_workers=[4,4],is_trains=[True, False],\n",
    "                                          collate_fns=[vqa_collate_fn,None])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.text_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "albef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
