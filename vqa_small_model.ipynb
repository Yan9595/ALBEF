{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a small model, train end-to-end using only VQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/miniconda3/envs/albef/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "\n",
    "# use vqa model\n",
    "from models.model_vqa import ALBEF\n",
    "\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "\n",
    "import utils\n",
    "from dataset.utils import save_result\n",
    "from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn\n",
    "\n",
    "from scheduler import create_scheduler\n",
    "from optim import create_optimizer\n",
    "\n",
    "# print and plotting \n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data, set True if you have unzipped the data\n",
    "unzipped = True\n",
    "VQA_DATA_DIR = 'data'\n",
    "# VQA_NEWMODEL_DIR = 'pretrained/vqa'\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(VQA_DATA_DIR, exist_ok=True)\n",
    "# os.makedirs(VQA_NEWMODEL_DIR, exist_ok=True)\n",
    "if not unzipped:\n",
    "    import os\n",
    "    import zipfile\n",
    "\n",
    "    # prep downloaded data\n",
    "\n",
    "\n",
    "    zip_files = [\n",
    "        'train2014.zip',\n",
    "        'test2015.zip',        \n",
    "        'val2014.zip',\n",
    "        'data.tar.gz'\n",
    "    ]\n",
    "\n",
    "    for zip_file in zip_files:\n",
    "        zip_path = os.path.join(VQA_DATA_DIR, zip_file)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(VQA_DATA_DIR)\n",
    "    print('Unzipped all files.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0,\n",
      " 'answer_list': 'data/answer_list.json',\n",
      " 'batch_size_test': 16,\n",
      " 'batch_size_train': 32,\n",
      " 'bert_config': 'configs/config_bert_small.json',\n",
      " 'distill': False,\n",
      " 'eos': '[SEP]',\n",
      " 'image_res': 224,\n",
      " 'k_test': 128,\n",
      " 'optimizer': {'lr': 2e-05, 'opt': 'adamW', 'weight_decay': 0.02},\n",
      " 'schedular': {'cooldown_epochs': 0,\n",
      "               'decay_rate': 1,\n",
      "               'epochs': 8,\n",
      "               'lr': 2e-05,\n",
      "               'min_lr': 1e-06,\n",
      "               'sched': 'cosine',\n",
      "               'warmup_epochs': 4,\n",
      "               'warmup_lr': 1e-05},\n",
      " 'test_file': ['data/vqa_test.json'],\n",
      " 'train_file': ['data/vqa_train.json', 'data/vqa_val.json'],\n",
      " 'vg_root': 'data/',\n",
      " 'vqa_root': 'data/',\n",
      " 'warm_up': False}\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "args = argparse.Namespace()\n",
    "args.config = './configs/VQA_only.yaml'\n",
    "args.checkpoint = '' # './ALBEF_4M.pth'\n",
    "args.output_dir = './output/vqa_end2end'\n",
    "args.evaluate = False # to train use False\n",
    "args.text_encoder = 'bert-base-uncased'\n",
    "args.text_decoder = 'bert-base-uncased'\n",
    "args.device = 'cuda'\n",
    "args.seed = 42\n",
    "args.distributed = False\n",
    "\n",
    "config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n",
    "pprint(config)\n",
    "\n",
    "# make result folder and save config\n",
    "args.result_dir = os.path.join(args.output_dir, 'result')\n",
    "\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training functions\n",
    "def train(model, data_loader, optimizer, tokenizer, epoch, warmup_steps, device, scheduler, config):\n",
    "    # train\n",
    "    model.train()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
    "\n",
    "    header = 'Train Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 50\n",
    "    step_size = 100\n",
    "    warmup_iterations = warmup_steps*step_size\n",
    "\n",
    "    for i,(image, question, answer, weights, n) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        image, weights = image.to(device,non_blocking=True), weights.to(device,non_blocking=True)\n",
    "        question_input = tokenizer(question, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\").to(device)\n",
    "        answer_input = tokenizer(answer, padding='longest', return_tensors=\"pt\").to(device)\n",
    "\n",
    "        if epoch>0 or not config['warm_up']:\n",
    "            alpha = config['alpha']\n",
    "        else:\n",
    "            alpha = config['alpha']*min(1,i/len(data_loader))\n",
    "\n",
    "        loss = model(image, question_input, answer_input, train=True, alpha=alpha, k=n, weights=weights)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        if epoch==0 and i%step_size==0 and i<=warmup_iterations:\n",
    "            scheduler.step(i//step_size)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger.global_avg())\n",
    "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader, tokenizer, device, config) :\n",
    "    # test\n",
    "    model.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Generate VQA test result:'\n",
    "    print_freq = 50\n",
    "\n",
    "    result = []\n",
    "\n",
    "    answer_list = [answer+config['eos'] for answer in data_loader.dataset.answer_list]\n",
    "    answer_input = tokenizer(answer_list, padding='longest', return_tensors='pt').to(device)\n",
    "\n",
    "    for n, (image, question, question_id) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        image = image.to(device,non_blocking=True)\n",
    "        question_input = tokenizer(question, padding='longest', return_tensors=\"pt\").to(device)\n",
    "\n",
    "        topk_ids, topk_probs = model(image, question_input, answer_input, train=False, k=config['k_test'])\n",
    "\n",
    "        for ques_id, topk_id, topk_prob in zip(question_id, topk_ids, topk_probs):\n",
    "            ques_id = int(ques_id.item())\n",
    "            _, pred = topk_prob.max(dim=0)\n",
    "            result.append({\"question_id\":ques_id, \"answer\":data_loader.dataset.answer_list[topk_id[pred]]})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n"
     ]
    }
   ],
   "source": [
    "# setup for training (from main)\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "start_epoch = 0\n",
    "max_epoch = config['schedular']['epochs']\n",
    "warmup_steps = config['schedular']['warmup_epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vqa datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/miniconda3/envs/albef/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# make dataset and dataloader\n",
    "print(\"Creating vqa datasets\")\n",
    "datasets = create_dataset('vqa', config)\n",
    "\n",
    "if args.distributed:\n",
    "    num_tasks = utils.get_world_size()\n",
    "    global_rank = utils.get_rank()\n",
    "    samplers = create_sampler(datasets, [True, False], num_tasks, global_rank)\n",
    "else:\n",
    "    samplers = [None, None]\n",
    "\n",
    "train_loader, test_loader = create_loader(datasets,samplers,\n",
    "                                          batch_size=[config['batch_size_train'],config['batch_size_test']],\n",
    "                                          num_workers=[4,4],is_trains=[True, False],\n",
    "                                          collate_fns=[vqa_collate_fn,None])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up GPU memory\n",
    "import gc\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "clear_gpu_memory()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ALBEF(\n",
       "  (visual_encoder): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3-5): 3 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 384)\n",
       "        (token_type_embeddings): Embedding(2, 384)\n",
       "        (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=384, out_features=30522, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Model ####\n",
    "print(\"Creating model\")\n",
    "model = ALBEF(config=config, text_encoder=args.text_encoder, text_decoder=args.text_decoder, tokenizer=tokenizer)\n",
    "model = model.to(device)\n",
    "\n",
    "arg_opt = utils.AttrDict(config['optimizer'])\n",
    "optimizer = create_optimizer(arg_opt, model)\n",
    "arg_sche = utils.AttrDict(config['schedular'])\n",
    "lr_scheduler, _ = create_scheduler(arg_sche, optimizer)\n",
    "\n",
    "# check model\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint path: ./output/vqa_end2end/checkpoint_00.pth. start epoch: 1\n"
     ]
    }
   ],
   "source": [
    "# revise the checkpoint to continue training (when error occurs)\n",
    "args.checkpoint = './output/vqa_end2end/checkpoint_00.pth' # './ALBEF_4M.pth'\n",
    "if not os.path.exists(args.checkpoint):\n",
    "    raise FileNotFoundError(f\"Checkpoint file '{args.checkpoint}' does not exist.\")\n",
    "# note: need to manually adjust the start epoch\n",
    "start_epoch = 1\n",
    "print(f'checkpoint path: {args.checkpoint}. start epoch: {start_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_encoder.encoder.layer.0.attention.self.query.weight\n",
      "['text_encoder', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'weight']\n",
      "attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111633/4256615453.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.checkpoint, map_location='cpu')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(encoder_keys)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(encoder_keys[\u001b[32m4\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m layer_num = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mencoder_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m layer_num<\u001b[32m6\u001b[39m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m state_dict[key]\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'attention'"
     ]
    }
   ],
   "source": [
    "# load check point to continue training\n",
    "if args.checkpoint:\n",
    "    checkpoint = torch.load(args.checkpoint, map_location='cpu')\n",
    "    if args.evaluate:\n",
    "        state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint['model']\n",
    "\n",
    "    # reshape positional embedding to accomodate for image resolution change\n",
    "    pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)\n",
    "    state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "\n",
    "    if not args.evaluate:\n",
    "        if config['distill']:\n",
    "            m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)\n",
    "            state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped\n",
    "\n",
    "        for key in list(state_dict.keys()):\n",
    "            if 'bert' in key:\n",
    "                encoder_key = key.replace('bert.','')\n",
    "                state_dict[encoder_key] = state_dict[key]\n",
    "            # intialize text decoder as multimodal encoder (last 6 layers of model.text_encoder)\n",
    "            if 'text_encoder' in key:\n",
    "                if 'layer' in key:\n",
    "                    # print(key)\n",
    "                    encoder_keys = key.split('.')\n",
    "                    # print(encoder_keys)\n",
    "                    # print(encoder_keys[4])\n",
    "                    tmp_fix_idx = 5 # for the downsized model, idx 5 is the layer number\n",
    "                    layer_num = int(encoder_keys[tmp_fix_idx]) # 4\n",
    "                    if layer_num<6:\n",
    "                        del state_dict[key]\n",
    "                        continue\n",
    "                    else:\n",
    "                        decoder_layer_num = (layer_num-6)\n",
    "                        encoder_keys[4] = str(decoder_layer_num)\n",
    "                        encoder_key = '.'.join(encoder_keys)\n",
    "                else:\n",
    "                    encoder_key = key\n",
    "                decoder_key = encoder_key.replace('text_encoder','text_decoder')\n",
    "                state_dict[decoder_key] = state_dict[key]\n",
    "\n",
    "                del state_dict[key]\n",
    "\n",
    "    msg = model.load_state_dict(state_dict,strict=False)\n",
    "    print('load checkpoint from %s'%args.checkpoint)\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle distributed training\n",
    "model_without_ddp = model\n",
    "if args.distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "    model_without_ddp = model.module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/Github/2025Spring/ALBEF/dataset/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
      "  offset = -low * scale\n",
      "/home/jason/Github/2025Spring/ALBEF/dataset/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
      "  offset = -low * scale\n",
      "/home/jason/Github/2025Spring/ALBEF/dataset/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
      "  offset = -low * scale\n",
      "/home/jason/Github/2025Spring/ALBEF/dataset/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
      "  offset = -low * scale\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: [0]  [    0/20565]  eta: 9:29:51  lr: 0.000010  loss: 23.6020  time: 1.6626  data: 0.3350  max mem: 1804\n",
      "Train Epoch: [0]  [   50/20565]  eta: 3:30:50  lr: 0.000010  loss: 16.5129  time: 0.5979  data: 0.0001  max mem: 2837\n",
      "Train Epoch: [0]  [  100/20565]  eta: 3:29:59  lr: 0.000010  loss: 13.5169  time: 0.6183  data: 0.0001  max mem: 2837\n",
      "Train Epoch: [0]  [  150/20565]  eta: 3:29:14  lr: 0.000013  loss: 14.0854  time: 0.6205  data: 0.0001  max mem: 2963\n",
      "Train Epoch: [0]  [  200/20565]  eta: 3:29:13  lr: 0.000013  loss: 11.9379  time: 0.6256  data: 0.0001  max mem: 2963\n",
      "Train Epoch: [0]  [  250/20565]  eta: 3:29:11  lr: 0.000015  loss: 12.3172  time: 0.6120  data: 0.0001  max mem: 2963\n",
      "Train Epoch: [0]  [  300/20565]  eta: 3:29:05  lr: 0.000015  loss: 11.6710  time: 0.6310  data: 0.0001  max mem: 2963\n",
      "Train Epoch: [0]  [  350/20565]  eta: 3:28:46  lr: 0.000018  loss: 10.4605  time: 0.6272  data: 0.0001  max mem: 3141\n",
      "Train Epoch: [0]  [  400/20565]  eta: 3:28:11  lr: 0.000018  loss: 10.5812  time: 0.6226  data: 0.0001  max mem: 3141\n",
      "Train Epoch: [0]  [  450/20565]  eta: 3:27:40  lr: 0.000020  loss: 6.7651  time: 0.6205  data: 0.0001  max mem: 3141\n",
      "Train Epoch: [0]  [  500/20565]  eta: 3:27:36  lr: 0.000020  loss: 7.8522  time: 0.6408  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  550/20565]  eta: 3:27:05  lr: 0.000020  loss: 10.2337  time: 0.6133  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  600/20565]  eta: 3:26:37  lr: 0.000020  loss: 9.4560  time: 0.6282  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  650/20565]  eta: 3:25:55  lr: 0.000020  loss: 8.8360  time: 0.6154  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  700/20565]  eta: 3:25:18  lr: 0.000020  loss: 7.3067  time: 0.6150  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  750/20565]  eta: 3:24:36  lr: 0.000020  loss: 9.0012  time: 0.6131  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  800/20565]  eta: 3:23:56  lr: 0.000020  loss: 6.8961  time: 0.6141  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  850/20565]  eta: 3:23:31  lr: 0.000020  loss: 7.3014  time: 0.6338  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  900/20565]  eta: 3:22:11  lr: 0.000020  loss: 8.7773  time: 0.6042  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [  950/20565]  eta: 3:21:50  lr: 0.000020  loss: 5.0542  time: 0.6230  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1000/20565]  eta: 3:21:22  lr: 0.000020  loss: 8.1839  time: 0.6084  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1050/20565]  eta: 3:20:40  lr: 0.000020  loss: 5.6339  time: 0.6099  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1100/20565]  eta: 3:19:53  lr: 0.000020  loss: 4.6654  time: 0.6001  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1150/20565]  eta: 3:19:13  lr: 0.000020  loss: 6.9434  time: 0.6114  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1200/20565]  eta: 3:18:29  lr: 0.000020  loss: 7.3225  time: 0.5994  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1250/20565]  eta: 3:17:48  lr: 0.000020  loss: 6.2601  time: 0.5971  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1300/20565]  eta: 3:17:07  lr: 0.000020  loss: 6.2162  time: 0.5969  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1350/20565]  eta: 3:16:27  lr: 0.000020  loss: 5.7576  time: 0.6039  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1400/20565]  eta: 3:15:55  lr: 0.000020  loss: 6.6400  time: 0.6089  data: 0.0001  max mem: 3194\n",
      "Train Epoch: [0]  [ 1450/20565]  eta: 3:15:24  lr: 0.000020  loss: 6.0300  time: 0.6148  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1500/20565]  eta: 3:15:02  lr: 0.000020  loss: 4.7406  time: 0.6232  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1550/20565]  eta: 3:14:29  lr: 0.000020  loss: 7.1544  time: 0.6103  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1600/20565]  eta: 3:13:55  lr: 0.000020  loss: 6.0427  time: 0.6067  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1650/20565]  eta: 3:13:24  lr: 0.000020  loss: 6.8598  time: 0.6147  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1700/20565]  eta: 3:12:52  lr: 0.000020  loss: 6.1603  time: 0.6170  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1750/20565]  eta: 3:12:22  lr: 0.000020  loss: 6.5586  time: 0.6149  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1800/20565]  eta: 3:11:48  lr: 0.000020  loss: 8.0858  time: 0.6077  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1850/20565]  eta: 3:11:19  lr: 0.000020  loss: 5.3977  time: 0.6109  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1900/20565]  eta: 3:10:48  lr: 0.000020  loss: 8.0315  time: 0.6124  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 1950/20565]  eta: 3:10:17  lr: 0.000020  loss: 6.7064  time: 0.6047  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2000/20565]  eta: 3:09:43  lr: 0.000020  loss: 3.3864  time: 0.6133  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2050/20565]  eta: 3:09:08  lr: 0.000020  loss: 3.8614  time: 0.6014  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2100/20565]  eta: 3:08:38  lr: 0.000020  loss: 7.5623  time: 0.6133  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2150/20565]  eta: 3:08:05  lr: 0.000020  loss: 5.8747  time: 0.6119  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2200/20565]  eta: 3:07:30  lr: 0.000020  loss: 6.7311  time: 0.6041  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2250/20565]  eta: 3:06:57  lr: 0.000020  loss: 5.6180  time: 0.6067  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2300/20565]  eta: 3:06:21  lr: 0.000020  loss: 5.3366  time: 0.5950  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2350/20565]  eta: 3:05:48  lr: 0.000020  loss: 6.3341  time: 0.6033  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2400/20565]  eta: 3:05:15  lr: 0.000020  loss: 6.0536  time: 0.6090  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2450/20565]  eta: 3:04:44  lr: 0.000020  loss: 7.5187  time: 0.6029  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2500/20565]  eta: 3:04:10  lr: 0.000020  loss: 5.1662  time: 0.6007  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2550/20565]  eta: 3:03:35  lr: 0.000020  loss: 4.9817  time: 0.6002  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2600/20565]  eta: 3:03:04  lr: 0.000020  loss: 5.7248  time: 0.6110  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2650/20565]  eta: 3:02:32  lr: 0.000020  loss: 5.9581  time: 0.6061  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2700/20565]  eta: 3:02:01  lr: 0.000020  loss: 4.2103  time: 0.6051  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2750/20565]  eta: 3:01:29  lr: 0.000020  loss: 5.8954  time: 0.6067  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2800/20565]  eta: 3:00:59  lr: 0.000020  loss: 6.9683  time: 0.6146  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2850/20565]  eta: 3:00:28  lr: 0.000020  loss: 5.4722  time: 0.6214  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2900/20565]  eta: 2:59:57  lr: 0.000020  loss: 9.0702  time: 0.6083  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 2950/20565]  eta: 2:59:25  lr: 0.000020  loss: 4.9913  time: 0.6065  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3000/20565]  eta: 2:58:53  lr: 0.000020  loss: 3.4674  time: 0.6058  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3050/20565]  eta: 2:58:23  lr: 0.000020  loss: 5.5461  time: 0.6169  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3100/20565]  eta: 2:57:35  lr: 0.000020  loss: 6.0840  time: 0.5261  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3150/20565]  eta: 2:56:43  lr: 0.000020  loss: 6.4423  time: 0.5315  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3200/20565]  eta: 2:55:52  lr: 0.000020  loss: 4.7671  time: 0.5297  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3250/20565]  eta: 2:55:00  lr: 0.000020  loss: 6.3955  time: 0.5332  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3300/20565]  eta: 2:54:10  lr: 0.000020  loss: 5.9679  time: 0.5288  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3350/20565]  eta: 2:53:20  lr: 0.000020  loss: 7.9673  time: 0.5237  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3400/20565]  eta: 2:52:31  lr: 0.000020  loss: 6.1722  time: 0.5347  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3450/20565]  eta: 2:51:42  lr: 0.000020  loss: 7.0813  time: 0.5268  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3500/20565]  eta: 2:50:56  lr: 0.000020  loss: 4.3177  time: 0.5381  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3550/20565]  eta: 2:50:07  lr: 0.000020  loss: 5.6618  time: 0.5216  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3600/20565]  eta: 2:49:21  lr: 0.000020  loss: 5.2398  time: 0.5276  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3650/20565]  eta: 2:48:36  lr: 0.000020  loss: 4.2935  time: 0.5253  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3700/20565]  eta: 2:47:51  lr: 0.000020  loss: 6.0252  time: 0.5346  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3750/20565]  eta: 2:47:06  lr: 0.000020  loss: 4.9888  time: 0.5322  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3800/20565]  eta: 2:46:21  lr: 0.000020  loss: 5.9211  time: 0.5240  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3850/20565]  eta: 2:45:37  lr: 0.000020  loss: 3.8925  time: 0.5284  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3900/20565]  eta: 2:44:54  lr: 0.000020  loss: 5.2935  time: 0.5315  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 3950/20565]  eta: 2:44:13  lr: 0.000020  loss: 2.9070  time: 0.5302  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4000/20565]  eta: 2:43:31  lr: 0.000020  loss: 6.5222  time: 0.5337  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4050/20565]  eta: 2:42:48  lr: 0.000020  loss: 4.2079  time: 0.5235  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4100/20565]  eta: 2:42:07  lr: 0.000020  loss: 6.4578  time: 0.5275  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4150/20565]  eta: 2:41:26  lr: 0.000020  loss: 5.1204  time: 0.5306  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4200/20565]  eta: 2:40:47  lr: 0.000020  loss: 5.9672  time: 0.5400  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4250/20565]  eta: 2:40:06  lr: 0.000020  loss: 6.2293  time: 0.5368  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4300/20565]  eta: 2:39:25  lr: 0.000020  loss: 4.1433  time: 0.5228  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4350/20565]  eta: 2:38:45  lr: 0.000020  loss: 5.6732  time: 0.5258  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4400/20565]  eta: 2:38:05  lr: 0.000020  loss: 4.8431  time: 0.5372  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4450/20565]  eta: 2:37:25  lr: 0.000020  loss: 4.9928  time: 0.5302  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4500/20565]  eta: 2:36:47  lr: 0.000020  loss: 5.7327  time: 0.5323  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4550/20565]  eta: 2:36:08  lr: 0.000020  loss: 5.6289  time: 0.5320  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4600/20565]  eta: 2:35:29  lr: 0.000020  loss: 6.3641  time: 0.5237  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4650/20565]  eta: 2:34:51  lr: 0.000020  loss: 5.9753  time: 0.5303  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4700/20565]  eta: 2:34:13  lr: 0.000020  loss: 6.6561  time: 0.5413  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4750/20565]  eta: 2:33:36  lr: 0.000020  loss: 4.3174  time: 0.5288  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4800/20565]  eta: 2:32:58  lr: 0.000020  loss: 4.2767  time: 0.5226  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4850/20565]  eta: 2:32:20  lr: 0.000020  loss: 4.1951  time: 0.5374  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4900/20565]  eta: 2:31:45  lr: 0.000020  loss: 5.7031  time: 0.5280  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 4950/20565]  eta: 2:31:08  lr: 0.000020  loss: 4.4304  time: 0.5343  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5000/20565]  eta: 2:30:31  lr: 0.000020  loss: 4.8686  time: 0.5285  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5050/20565]  eta: 2:29:54  lr: 0.000020  loss: 4.7189  time: 0.5308  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5100/20565]  eta: 2:29:18  lr: 0.000020  loss: 4.8124  time: 0.5369  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5150/20565]  eta: 2:28:42  lr: 0.000020  loss: 5.9005  time: 0.5291  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5200/20565]  eta: 2:28:06  lr: 0.000020  loss: 5.9268  time: 0.5360  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5250/20565]  eta: 2:27:30  lr: 0.000020  loss: 6.5017  time: 0.5338  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5300/20565]  eta: 2:26:54  lr: 0.000020  loss: 6.3017  time: 0.5230  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5350/20565]  eta: 2:26:19  lr: 0.000020  loss: 5.7582  time: 0.5382  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5400/20565]  eta: 2:25:44  lr: 0.000020  loss: 4.6931  time: 0.5237  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5450/20565]  eta: 2:25:08  lr: 0.000020  loss: 5.4513  time: 0.5306  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5500/20565]  eta: 2:24:33  lr: 0.000020  loss: 4.4824  time: 0.5312  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5550/20565]  eta: 2:23:58  lr: 0.000020  loss: 5.6206  time: 0.5337  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5600/20565]  eta: 2:23:24  lr: 0.000020  loss: 5.9516  time: 0.5289  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5650/20565]  eta: 2:22:48  lr: 0.000020  loss: 3.7358  time: 0.5272  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5700/20565]  eta: 2:22:14  lr: 0.000020  loss: 5.7713  time: 0.5349  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5750/20565]  eta: 2:21:40  lr: 0.000020  loss: 3.9321  time: 0.5343  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5800/20565]  eta: 2:21:07  lr: 0.000020  loss: 4.5540  time: 0.5308  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5850/20565]  eta: 2:20:33  lr: 0.000020  loss: 6.1626  time: 0.5391  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5900/20565]  eta: 2:19:59  lr: 0.000020  loss: 3.9960  time: 0.5238  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 5950/20565]  eta: 2:19:25  lr: 0.000020  loss: 5.1411  time: 0.5300  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6000/20565]  eta: 2:18:51  lr: 0.000020  loss: 7.1486  time: 0.5280  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6050/20565]  eta: 2:18:17  lr: 0.000020  loss: 5.9547  time: 0.5312  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6100/20565]  eta: 2:17:44  lr: 0.000020  loss: 6.5955  time: 0.5255  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6150/20565]  eta: 2:17:11  lr: 0.000020  loss: 3.8502  time: 0.5314  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6200/20565]  eta: 2:16:38  lr: 0.000020  loss: 5.6487  time: 0.5353  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6250/20565]  eta: 2:16:05  lr: 0.000020  loss: 6.5205  time: 0.5312  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6300/20565]  eta: 2:15:32  lr: 0.000020  loss: 4.5624  time: 0.5344  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6350/20565]  eta: 2:14:59  lr: 0.000020  loss: 6.9288  time: 0.5272  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6400/20565]  eta: 2:14:27  lr: 0.000020  loss: 5.9149  time: 0.5310  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6450/20565]  eta: 2:13:54  lr: 0.000020  loss: 7.5379  time: 0.5381  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6500/20565]  eta: 2:13:21  lr: 0.000020  loss: 5.2916  time: 0.5296  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6550/20565]  eta: 2:12:49  lr: 0.000020  loss: 3.8290  time: 0.5291  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6600/20565]  eta: 2:12:16  lr: 0.000020  loss: 4.2332  time: 0.5314  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6650/20565]  eta: 2:11:44  lr: 0.000020  loss: 5.9250  time: 0.5297  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6700/20565]  eta: 2:11:12  lr: 0.000020  loss: 6.2814  time: 0.5255  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6750/20565]  eta: 2:10:40  lr: 0.000020  loss: 6.1063  time: 0.5313  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6800/20565]  eta: 2:10:08  lr: 0.000020  loss: 4.9065  time: 0.5249  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6850/20565]  eta: 2:09:36  lr: 0.000020  loss: 5.1535  time: 0.5315  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6900/20565]  eta: 2:09:04  lr: 0.000020  loss: 5.7594  time: 0.5357  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 6950/20565]  eta: 2:08:32  lr: 0.000020  loss: 5.1374  time: 0.5354  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7000/20565]  eta: 2:08:01  lr: 0.000020  loss: 4.6940  time: 0.5358  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7050/20565]  eta: 2:07:29  lr: 0.000020  loss: 4.8745  time: 0.5378  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7100/20565]  eta: 2:06:58  lr: 0.000020  loss: 4.5137  time: 0.5394  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7150/20565]  eta: 2:06:26  lr: 0.000020  loss: 4.1856  time: 0.5345  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7200/20565]  eta: 2:05:55  lr: 0.000020  loss: 4.1698  time: 0.5313  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7250/20565]  eta: 2:05:24  lr: 0.000020  loss: 3.9671  time: 0.5368  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7300/20565]  eta: 2:04:52  lr: 0.000020  loss: 6.0812  time: 0.5314  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7350/20565]  eta: 2:04:21  lr: 0.000020  loss: 4.6425  time: 0.5306  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7400/20565]  eta: 2:03:49  lr: 0.000020  loss: 3.7939  time: 0.5321  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7450/20565]  eta: 2:03:19  lr: 0.000020  loss: 3.7887  time: 0.5321  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7500/20565]  eta: 2:02:48  lr: 0.000020  loss: 3.0118  time: 0.5283  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7550/20565]  eta: 2:02:17  lr: 0.000020  loss: 7.5617  time: 0.5324  data: 0.0001  max mem: 3243\n",
      "Train Epoch: [0]  [ 7600/20565]  eta: 2:01:46  lr: 0.000020  loss: 6.3500  time: 0.5395  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 7650/20565]  eta: 2:01:15  lr: 0.000020  loss: 5.0989  time: 0.5381  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 7700/20565]  eta: 2:00:45  lr: 0.000020  loss: 4.9673  time: 0.5336  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 7750/20565]  eta: 2:00:14  lr: 0.000020  loss: 3.2917  time: 0.5349  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 7800/20565]  eta: 1:59:43  lr: 0.000020  loss: 4.4856  time: 0.5292  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 7850/20565]  eta: 1:59:12  lr: 0.000020  loss: 3.8632  time: 0.5282  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 7900/20565]  eta: 1:58:41  lr: 0.000020  loss: 3.8350  time: 0.5203  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 7950/20565]  eta: 1:58:10  lr: 0.000020  loss: 5.7001  time: 0.5253  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8000/20565]  eta: 1:57:40  lr: 0.000020  loss: 3.4541  time: 0.5228  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8050/20565]  eta: 1:57:09  lr: 0.000020  loss: 4.5348  time: 0.5315  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8100/20565]  eta: 1:56:38  lr: 0.000020  loss: 6.5010  time: 0.5281  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8150/20565]  eta: 1:56:08  lr: 0.000020  loss: 3.6294  time: 0.5290  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8200/20565]  eta: 1:55:38  lr: 0.000020  loss: 3.2587  time: 0.5313  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8250/20565]  eta: 1:55:07  lr: 0.000020  loss: 2.7257  time: 0.5270  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8300/20565]  eta: 1:54:37  lr: 0.000020  loss: 4.0847  time: 0.5398  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8350/20565]  eta: 1:54:07  lr: 0.000020  loss: 4.3760  time: 0.5232  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8400/20565]  eta: 1:53:36  lr: 0.000020  loss: 4.2998  time: 0.5252  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8450/20565]  eta: 1:53:06  lr: 0.000020  loss: 5.0268  time: 0.5286  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8500/20565]  eta: 1:52:37  lr: 0.000020  loss: 5.7696  time: 0.5339  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8550/20565]  eta: 1:52:07  lr: 0.000020  loss: 5.8422  time: 0.5260  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8600/20565]  eta: 1:51:37  lr: 0.000020  loss: 5.4676  time: 0.5345  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8650/20565]  eta: 1:51:06  lr: 0.000020  loss: 3.9664  time: 0.5289  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8700/20565]  eta: 1:50:36  lr: 0.000020  loss: 5.7337  time: 0.5259  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8750/20565]  eta: 1:50:07  lr: 0.000020  loss: 5.1872  time: 0.5283  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8800/20565]  eta: 1:49:37  lr: 0.000020  loss: 3.1453  time: 0.5285  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8850/20565]  eta: 1:49:07  lr: 0.000020  loss: 3.1120  time: 0.5337  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8900/20565]  eta: 1:48:37  lr: 0.000020  loss: 5.1378  time: 0.5299  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 8950/20565]  eta: 1:48:07  lr: 0.000020  loss: 3.4132  time: 0.5341  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9000/20565]  eta: 1:47:37  lr: 0.000020  loss: 5.3741  time: 0.5344  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9050/20565]  eta: 1:47:08  lr: 0.000020  loss: 5.7511  time: 0.5375  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9100/20565]  eta: 1:46:39  lr: 0.000020  loss: 3.7662  time: 0.5419  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9150/20565]  eta: 1:46:10  lr: 0.000020  loss: 5.7672  time: 0.5413  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9200/20565]  eta: 1:45:40  lr: 0.000020  loss: 5.6454  time: 0.5288  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9250/20565]  eta: 1:45:10  lr: 0.000020  loss: 4.7655  time: 0.5309  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9300/20565]  eta: 1:44:41  lr: 0.000020  loss: 5.1459  time: 0.5368  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9350/20565]  eta: 1:44:11  lr: 0.000020  loss: 3.5420  time: 0.5358  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9400/20565]  eta: 1:43:42  lr: 0.000020  loss: 4.6498  time: 0.5255  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9450/20565]  eta: 1:43:13  lr: 0.000020  loss: 5.0168  time: 0.5371  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9500/20565]  eta: 1:42:43  lr: 0.000020  loss: 5.4546  time: 0.5335  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9550/20565]  eta: 1:42:14  lr: 0.000020  loss: 3.6681  time: 0.5260  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9600/20565]  eta: 1:41:45  lr: 0.000020  loss: 4.4235  time: 0.5432  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9650/20565]  eta: 1:41:15  lr: 0.000020  loss: 3.7554  time: 0.5371  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9700/20565]  eta: 1:40:46  lr: 0.000020  loss: 5.5228  time: 0.5324  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9750/20565]  eta: 1:40:17  lr: 0.000020  loss: 3.8279  time: 0.5321  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9800/20565]  eta: 1:39:48  lr: 0.000020  loss: 4.8905  time: 0.5213  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9850/20565]  eta: 1:39:19  lr: 0.000020  loss: 2.8774  time: 0.5331  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9900/20565]  eta: 1:38:50  lr: 0.000020  loss: 7.0284  time: 0.5308  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [ 9950/20565]  eta: 1:38:21  lr: 0.000020  loss: 4.0764  time: 0.5360  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10000/20565]  eta: 1:37:52  lr: 0.000020  loss: 5.8745  time: 0.5256  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10050/20565]  eta: 1:37:22  lr: 0.000020  loss: 2.9081  time: 0.5305  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10100/20565]  eta: 1:36:53  lr: 0.000020  loss: 5.4673  time: 0.5242  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10150/20565]  eta: 1:36:24  lr: 0.000020  loss: 4.6078  time: 0.5292  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10200/20565]  eta: 1:35:55  lr: 0.000020  loss: 4.7424  time: 0.5316  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10250/20565]  eta: 1:35:26  lr: 0.000020  loss: 3.9266  time: 0.5226  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10300/20565]  eta: 1:34:57  lr: 0.000020  loss: 5.5680  time: 0.5362  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10350/20565]  eta: 1:34:28  lr: 0.000020  loss: 5.7129  time: 0.5260  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10400/20565]  eta: 1:33:59  lr: 0.000020  loss: 3.8334  time: 0.5306  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10450/20565]  eta: 1:33:30  lr: 0.000020  loss: 7.0419  time: 0.5340  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10500/20565]  eta: 1:33:02  lr: 0.000020  loss: 4.3660  time: 0.5317  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10550/20565]  eta: 1:32:33  lr: 0.000020  loss: 4.9464  time: 0.5346  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10600/20565]  eta: 1:32:04  lr: 0.000020  loss: 5.8641  time: 0.5342  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10650/20565]  eta: 1:31:35  lr: 0.000020  loss: 3.4581  time: 0.5318  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10700/20565]  eta: 1:31:06  lr: 0.000020  loss: 4.6072  time: 0.5321  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10750/20565]  eta: 1:30:38  lr: 0.000020  loss: 4.8274  time: 0.5268  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10800/20565]  eta: 1:30:09  lr: 0.000020  loss: 4.6523  time: 0.5322  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10850/20565]  eta: 1:29:40  lr: 0.000020  loss: 3.0273  time: 0.5196  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10900/20565]  eta: 1:29:11  lr: 0.000020  loss: 3.6289  time: 0.5303  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [10950/20565]  eta: 1:28:42  lr: 0.000020  loss: 4.2081  time: 0.5284  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11000/20565]  eta: 1:28:14  lr: 0.000020  loss: 5.4235  time: 0.5364  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11050/20565]  eta: 1:27:45  lr: 0.000020  loss: 4.1907  time: 0.5312  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11100/20565]  eta: 1:27:17  lr: 0.000020  loss: 3.9413  time: 0.5340  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11150/20565]  eta: 1:26:48  lr: 0.000020  loss: 6.2188  time: 0.5286  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11200/20565]  eta: 1:26:20  lr: 0.000020  loss: 4.9573  time: 0.5298  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11250/20565]  eta: 1:25:51  lr: 0.000020  loss: 4.8224  time: 0.5284  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11300/20565]  eta: 1:25:22  lr: 0.000020  loss: 3.7800  time: 0.5366  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11350/20565]  eta: 1:24:54  lr: 0.000020  loss: 6.4884  time: 0.5305  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11400/20565]  eta: 1:24:25  lr: 0.000020  loss: 4.5333  time: 0.5193  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11450/20565]  eta: 1:23:57  lr: 0.000020  loss: 4.3988  time: 0.5281  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11500/20565]  eta: 1:23:28  lr: 0.000020  loss: 5.3505  time: 0.5284  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11550/20565]  eta: 1:23:00  lr: 0.000020  loss: 5.4699  time: 0.5396  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11600/20565]  eta: 1:22:32  lr: 0.000020  loss: 3.9879  time: 0.5291  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11650/20565]  eta: 1:22:03  lr: 0.000020  loss: 5.0692  time: 0.5300  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11700/20565]  eta: 1:21:35  lr: 0.000020  loss: 4.4665  time: 0.5305  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11750/20565]  eta: 1:21:06  lr: 0.000020  loss: 3.5256  time: 0.5312  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11800/20565]  eta: 1:20:38  lr: 0.000020  loss: 4.3980  time: 0.5270  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11850/20565]  eta: 1:20:10  lr: 0.000020  loss: 5.1336  time: 0.5361  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11900/20565]  eta: 1:19:42  lr: 0.000020  loss: 4.9123  time: 0.5351  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [11950/20565]  eta: 1:19:13  lr: 0.000020  loss: 4.9183  time: 0.5444  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12000/20565]  eta: 1:18:45  lr: 0.000020  loss: 5.8122  time: 0.5283  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12050/20565]  eta: 1:18:17  lr: 0.000020  loss: 3.9585  time: 0.5267  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12100/20565]  eta: 1:17:49  lr: 0.000020  loss: 3.9403  time: 0.5327  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12150/20565]  eta: 1:17:20  lr: 0.000020  loss: 4.8827  time: 0.5275  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12200/20565]  eta: 1:16:52  lr: 0.000020  loss: 4.3898  time: 0.5285  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12250/20565]  eta: 1:16:24  lr: 0.000020  loss: 5.9847  time: 0.5294  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12300/20565]  eta: 1:15:56  lr: 0.000020  loss: 4.6041  time: 0.5311  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12350/20565]  eta: 1:15:28  lr: 0.000020  loss: 5.0082  time: 0.5335  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12400/20565]  eta: 1:14:59  lr: 0.000020  loss: 4.6800  time: 0.5238  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12450/20565]  eta: 1:14:31  lr: 0.000020  loss: 4.6891  time: 0.5225  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12500/20565]  eta: 1:14:03  lr: 0.000020  loss: 5.3067  time: 0.5340  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12550/20565]  eta: 1:13:34  lr: 0.000020  loss: 4.2046  time: 0.5339  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12600/20565]  eta: 1:13:06  lr: 0.000020  loss: 4.5133  time: 0.5321  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12650/20565]  eta: 1:12:38  lr: 0.000020  loss: 5.1855  time: 0.5395  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12700/20565]  eta: 1:12:10  lr: 0.000020  loss: 4.3658  time: 0.5296  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12750/20565]  eta: 1:11:42  lr: 0.000020  loss: 3.6365  time: 0.5314  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12800/20565]  eta: 1:11:14  lr: 0.000020  loss: 4.7719  time: 0.5376  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12850/20565]  eta: 1:10:46  lr: 0.000020  loss: 2.4399  time: 0.5356  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12900/20565]  eta: 1:10:18  lr: 0.000020  loss: 4.6177  time: 0.5265  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [12950/20565]  eta: 1:09:50  lr: 0.000020  loss: 4.3520  time: 0.5346  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13000/20565]  eta: 1:09:22  lr: 0.000020  loss: 5.5411  time: 0.5315  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13050/20565]  eta: 1:08:54  lr: 0.000020  loss: 4.1414  time: 0.5265  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13100/20565]  eta: 1:08:26  lr: 0.000020  loss: 6.6063  time: 0.5333  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13150/20565]  eta: 1:07:57  lr: 0.000020  loss: 5.2591  time: 0.5291  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13200/20565]  eta: 1:07:29  lr: 0.000020  loss: 4.8313  time: 0.5393  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13250/20565]  eta: 1:07:01  lr: 0.000020  loss: 4.6765  time: 0.5324  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13300/20565]  eta: 1:06:33  lr: 0.000020  loss: 4.9903  time: 0.5388  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13350/20565]  eta: 1:06:06  lr: 0.000020  loss: 4.8766  time: 0.5415  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13400/20565]  eta: 1:05:37  lr: 0.000020  loss: 5.1037  time: 0.5310  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13450/20565]  eta: 1:05:10  lr: 0.000020  loss: 6.9307  time: 0.5461  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13500/20565]  eta: 1:04:42  lr: 0.000020  loss: 3.0836  time: 0.5341  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13550/20565]  eta: 1:04:14  lr: 0.000020  loss: 4.6671  time: 0.5289  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13600/20565]  eta: 1:03:46  lr: 0.000020  loss: 3.4851  time: 0.5342  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13650/20565]  eta: 1:03:18  lr: 0.000020  loss: 3.6676  time: 0.5321  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13700/20565]  eta: 1:02:50  lr: 0.000020  loss: 5.2048  time: 0.5404  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13750/20565]  eta: 1:02:22  lr: 0.000020  loss: 4.1754  time: 0.5301  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13800/20565]  eta: 1:01:54  lr: 0.000020  loss: 5.5382  time: 0.5238  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13850/20565]  eta: 1:01:26  lr: 0.000020  loss: 3.8091  time: 0.5240  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13900/20565]  eta: 1:00:59  lr: 0.000020  loss: 3.7311  time: 0.5289  data: 0.0001  max mem: 3424\n",
      "Train Epoch: [0]  [13950/20565]  eta: 1:00:31  lr: 0.000020  loss: 3.2432  time: 0.5481  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14000/20565]  eta: 1:00:03  lr: 0.000020  loss: 2.9577  time: 0.5277  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14050/20565]  eta: 0:59:35  lr: 0.000020  loss: 3.4562  time: 0.5386  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14100/20565]  eta: 0:59:07  lr: 0.000020  loss: 4.1552  time: 0.5355  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14150/20565]  eta: 0:58:40  lr: 0.000020  loss: 5.5059  time: 0.5338  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14200/20565]  eta: 0:58:12  lr: 0.000020  loss: 5.9647  time: 0.5443  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14250/20565]  eta: 0:57:44  lr: 0.000020  loss: 3.5719  time: 0.5265  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14300/20565]  eta: 0:57:16  lr: 0.000020  loss: 4.5910  time: 0.5361  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14350/20565]  eta: 0:56:49  lr: 0.000020  loss: 3.3592  time: 0.5242  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14400/20565]  eta: 0:56:21  lr: 0.000020  loss: 2.9949  time: 0.5271  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14450/20565]  eta: 0:55:53  lr: 0.000020  loss: 4.0480  time: 0.5339  data: 0.0001  max mem: 3427\n",
      "Train Epoch: [0]  [14500/20565]  eta: 0:55:25  lr: 0.000020  loss: 5.2838  time: 0.5349  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14550/20565]  eta: 0:54:58  lr: 0.000020  loss: 6.1945  time: 0.5291  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14600/20565]  eta: 0:54:30  lr: 0.000020  loss: 3.6756  time: 0.5358  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14650/20565]  eta: 0:54:02  lr: 0.000020  loss: 4.1988  time: 0.5347  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14700/20565]  eta: 0:53:34  lr: 0.000020  loss: 5.3780  time: 0.5326  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14750/20565]  eta: 0:53:07  lr: 0.000020  loss: 4.8183  time: 0.5292  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14800/20565]  eta: 0:52:39  lr: 0.000020  loss: 3.3377  time: 0.5272  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14850/20565]  eta: 0:52:11  lr: 0.000020  loss: 3.5935  time: 0.5287  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14900/20565]  eta: 0:51:43  lr: 0.000020  loss: 4.0917  time: 0.5345  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [14950/20565]  eta: 0:51:16  lr: 0.000020  loss: 4.4366  time: 0.5290  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15000/20565]  eta: 0:50:48  lr: 0.000020  loss: 5.3892  time: 0.5300  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15050/20565]  eta: 0:50:20  lr: 0.000020  loss: 6.2425  time: 0.5478  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15100/20565]  eta: 0:49:53  lr: 0.000020  loss: 5.1158  time: 0.5364  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15150/20565]  eta: 0:49:25  lr: 0.000020  loss: 4.9728  time: 0.5304  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15200/20565]  eta: 0:48:58  lr: 0.000020  loss: 4.2268  time: 0.5336  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15250/20565]  eta: 0:48:30  lr: 0.000020  loss: 4.6188  time: 0.5271  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15300/20565]  eta: 0:48:02  lr: 0.000020  loss: 4.3288  time: 0.5295  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15350/20565]  eta: 0:47:34  lr: 0.000020  loss: 3.9724  time: 0.5376  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15400/20565]  eta: 0:47:07  lr: 0.000020  loss: 4.2453  time: 0.5317  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15450/20565]  eta: 0:46:39  lr: 0.000020  loss: 4.7348  time: 0.5321  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15500/20565]  eta: 0:46:12  lr: 0.000020  loss: 5.0270  time: 0.5291  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15550/20565]  eta: 0:45:44  lr: 0.000020  loss: 3.8674  time: 0.5257  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15600/20565]  eta: 0:45:16  lr: 0.000020  loss: 3.9856  time: 0.5399  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15650/20565]  eta: 0:44:49  lr: 0.000020  loss: 4.6038  time: 0.5280  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15700/20565]  eta: 0:44:21  lr: 0.000020  loss: 3.2200  time: 0.5389  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15750/20565]  eta: 0:43:53  lr: 0.000020  loss: 3.1133  time: 0.5285  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15800/20565]  eta: 0:43:26  lr: 0.000020  loss: 3.2383  time: 0.5334  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15850/20565]  eta: 0:42:58  lr: 0.000020  loss: 3.7004  time: 0.5243  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15900/20565]  eta: 0:42:31  lr: 0.000020  loss: 4.7652  time: 0.5317  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [15950/20565]  eta: 0:42:03  lr: 0.000020  loss: 3.2494  time: 0.5304  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16000/20565]  eta: 0:41:36  lr: 0.000020  loss: 6.1006  time: 0.5401  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16050/20565]  eta: 0:41:08  lr: 0.000020  loss: 4.9418  time: 0.5310  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16100/20565]  eta: 0:40:40  lr: 0.000020  loss: 2.9335  time: 0.5304  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16150/20565]  eta: 0:40:13  lr: 0.000020  loss: 3.9995  time: 0.5237  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16200/20565]  eta: 0:39:45  lr: 0.000020  loss: 3.3063  time: 0.5419  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16250/20565]  eta: 0:39:18  lr: 0.000020  loss: 5.4689  time: 0.5276  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16300/20565]  eta: 0:38:50  lr: 0.000020  loss: 3.4595  time: 0.5331  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16350/20565]  eta: 0:38:23  lr: 0.000020  loss: 3.8287  time: 0.5304  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16400/20565]  eta: 0:37:55  lr: 0.000020  loss: 3.6952  time: 0.5286  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16450/20565]  eta: 0:37:28  lr: 0.000020  loss: 4.2417  time: 0.5274  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16500/20565]  eta: 0:37:00  lr: 0.000020  loss: 4.2302  time: 0.5333  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16550/20565]  eta: 0:36:33  lr: 0.000020  loss: 5.8301  time: 0.5404  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16600/20565]  eta: 0:36:05  lr: 0.000020  loss: 3.8825  time: 0.5256  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16650/20565]  eta: 0:35:38  lr: 0.000020  loss: 6.2397  time: 0.5413  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16700/20565]  eta: 0:35:10  lr: 0.000020  loss: 4.0985  time: 0.5337  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16750/20565]  eta: 0:34:43  lr: 0.000020  loss: 5.2418  time: 0.5246  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16800/20565]  eta: 0:34:15  lr: 0.000020  loss: 2.7892  time: 0.5323  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16850/20565]  eta: 0:33:48  lr: 0.000020  loss: 3.8199  time: 0.5290  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16900/20565]  eta: 0:33:21  lr: 0.000020  loss: 4.6385  time: 0.5339  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [16950/20565]  eta: 0:32:53  lr: 0.000020  loss: 4.3075  time: 0.5323  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17000/20565]  eta: 0:32:26  lr: 0.000020  loss: 3.4231  time: 0.5355  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17050/20565]  eta: 0:31:58  lr: 0.000020  loss: 4.2205  time: 0.5359  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17100/20565]  eta: 0:31:31  lr: 0.000020  loss: 3.7617  time: 0.5398  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17150/20565]  eta: 0:31:03  lr: 0.000020  loss: 2.4203  time: 0.5301  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17200/20565]  eta: 0:30:36  lr: 0.000020  loss: 4.6739  time: 0.5350  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17250/20565]  eta: 0:30:09  lr: 0.000020  loss: 5.9448  time: 0.5286  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17300/20565]  eta: 0:29:41  lr: 0.000020  loss: 5.6430  time: 0.5311  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17350/20565]  eta: 0:29:14  lr: 0.000020  loss: 3.8631  time: 0.5288  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17400/20565]  eta: 0:28:46  lr: 0.000020  loss: 3.6081  time: 0.5229  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17450/20565]  eta: 0:28:19  lr: 0.000020  loss: 5.3486  time: 0.5245  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17500/20565]  eta: 0:27:52  lr: 0.000020  loss: 4.5771  time: 0.5317  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17550/20565]  eta: 0:27:24  lr: 0.000020  loss: 3.4202  time: 0.5380  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17600/20565]  eta: 0:26:57  lr: 0.000020  loss: 4.3232  time: 0.5338  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17650/20565]  eta: 0:26:29  lr: 0.000020  loss: 4.2269  time: 0.5289  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17700/20565]  eta: 0:26:02  lr: 0.000020  loss: 3.4245  time: 0.5240  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17750/20565]  eta: 0:25:35  lr: 0.000020  loss: 4.6802  time: 0.5338  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17800/20565]  eta: 0:25:07  lr: 0.000020  loss: 3.9654  time: 0.5296  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17850/20565]  eta: 0:24:40  lr: 0.000020  loss: 5.9312  time: 0.5385  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17900/20565]  eta: 0:24:13  lr: 0.000020  loss: 4.1666  time: 0.5275  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [17950/20565]  eta: 0:23:45  lr: 0.000020  loss: 5.2222  time: 0.5355  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18000/20565]  eta: 0:23:18  lr: 0.000020  loss: 4.9522  time: 0.5303  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18050/20565]  eta: 0:22:51  lr: 0.000020  loss: 4.8389  time: 0.5253  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18100/20565]  eta: 0:22:23  lr: 0.000020  loss: 3.7504  time: 0.5284  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18150/20565]  eta: 0:21:56  lr: 0.000020  loss: 4.1550  time: 0.5276  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18200/20565]  eta: 0:21:29  lr: 0.000020  loss: 2.9608  time: 0.5313  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18250/20565]  eta: 0:21:01  lr: 0.000020  loss: 5.9371  time: 0.5389  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18300/20565]  eta: 0:20:34  lr: 0.000020  loss: 4.8500  time: 0.5442  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18350/20565]  eta: 0:20:07  lr: 0.000020  loss: 4.2466  time: 0.5268  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18400/20565]  eta: 0:19:39  lr: 0.000020  loss: 3.3910  time: 0.5330  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18450/20565]  eta: 0:19:12  lr: 0.000020  loss: 4.5799  time: 0.5252  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18500/20565]  eta: 0:18:45  lr: 0.000020  loss: 4.1127  time: 0.5294  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18550/20565]  eta: 0:18:17  lr: 0.000020  loss: 5.5134  time: 0.5326  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18600/20565]  eta: 0:17:50  lr: 0.000020  loss: 5.2270  time: 0.5212  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18650/20565]  eta: 0:17:23  lr: 0.000020  loss: 3.5697  time: 0.5332  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18700/20565]  eta: 0:16:55  lr: 0.000020  loss: 4.5800  time: 0.5257  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18750/20565]  eta: 0:16:28  lr: 0.000020  loss: 4.5185  time: 0.5385  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18800/20565]  eta: 0:16:01  lr: 0.000020  loss: 5.0875  time: 0.5367  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18850/20565]  eta: 0:15:33  lr: 0.000020  loss: 4.0657  time: 0.5305  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18900/20565]  eta: 0:15:06  lr: 0.000020  loss: 2.8495  time: 0.5292  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [18950/20565]  eta: 0:14:39  lr: 0.000020  loss: 4.2677  time: 0.5342  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19000/20565]  eta: 0:14:12  lr: 0.000020  loss: 4.1552  time: 0.5387  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19050/20565]  eta: 0:13:44  lr: 0.000020  loss: 4.6637  time: 0.5420  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19100/20565]  eta: 0:13:17  lr: 0.000020  loss: 5.4545  time: 0.5379  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19150/20565]  eta: 0:12:50  lr: 0.000020  loss: 4.4116  time: 0.5401  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19200/20565]  eta: 0:12:23  lr: 0.000020  loss: 5.0727  time: 0.5273  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19250/20565]  eta: 0:11:55  lr: 0.000020  loss: 4.5228  time: 0.5291  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19300/20565]  eta: 0:11:28  lr: 0.000020  loss: 4.5765  time: 0.5404  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19350/20565]  eta: 0:11:01  lr: 0.000020  loss: 5.7365  time: 0.5302  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19400/20565]  eta: 0:10:34  lr: 0.000020  loss: 3.4207  time: 0.5353  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19450/20565]  eta: 0:10:06  lr: 0.000020  loss: 5.4357  time: 0.5341  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19500/20565]  eta: 0:09:39  lr: 0.000020  loss: 4.2712  time: 0.5275  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19550/20565]  eta: 0:09:12  lr: 0.000020  loss: 3.8942  time: 0.5409  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19600/20565]  eta: 0:08:45  lr: 0.000020  loss: 4.7836  time: 0.5318  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19650/20565]  eta: 0:08:17  lr: 0.000020  loss: 4.6966  time: 0.5248  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19700/20565]  eta: 0:07:50  lr: 0.000020  loss: 3.9514  time: 0.5300  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19750/20565]  eta: 0:07:23  lr: 0.000020  loss: 3.5668  time: 0.5250  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19800/20565]  eta: 0:06:56  lr: 0.000020  loss: 4.9631  time: 0.5319  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19850/20565]  eta: 0:06:28  lr: 0.000020  loss: 5.3782  time: 0.5314  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19900/20565]  eta: 0:06:01  lr: 0.000020  loss: 3.1151  time: 0.5378  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [19950/20565]  eta: 0:05:34  lr: 0.000020  loss: 4.3889  time: 0.5324  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20000/20565]  eta: 0:05:07  lr: 0.000020  loss: 4.9129  time: 0.5332  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20050/20565]  eta: 0:04:40  lr: 0.000020  loss: 4.5851  time: 0.5323  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20100/20565]  eta: 0:04:12  lr: 0.000020  loss: 5.3886  time: 0.5255  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20150/20565]  eta: 0:03:45  lr: 0.000020  loss: 4.8441  time: 0.5342  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20200/20565]  eta: 0:03:18  lr: 0.000020  loss: 4.3868  time: 0.5346  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20250/20565]  eta: 0:02:51  lr: 0.000020  loss: 4.1011  time: 0.5395  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20300/20565]  eta: 0:02:24  lr: 0.000020  loss: 3.1340  time: 0.5355  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20350/20565]  eta: 0:01:56  lr: 0.000020  loss: 4.6999  time: 0.5347  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20400/20565]  eta: 0:01:29  lr: 0.000020  loss: 3.1030  time: 0.5342  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20450/20565]  eta: 0:01:02  lr: 0.000020  loss: 4.5700  time: 0.5273  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20500/20565]  eta: 0:00:35  lr: 0.000020  loss: 4.4208  time: 0.5286  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20550/20565]  eta: 0:00:08  lr: 0.000020  loss: 3.9493  time: 0.5241  data: 0.0001  max mem: 3739\n",
      "Train Epoch: [0]  [20564/20565]  eta: 0:00:00  lr: 0.000020  loss: 4.6855  time: 0.5292  data: 0.0013  max mem: 3739\n",
      "Train Epoch: [0] Total time: 3:06:18 (0.5436 s / it)\n",
      "Averaged stats: lr: 0.0000  loss: 5.0924\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     25\u001b[39m     save_obj = {\n\u001b[32m     26\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: model_without_ddp.state_dict(),\n\u001b[32m     27\u001b[39m         \u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: optimizer.state_dict(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m: epoch,\n\u001b[32m     31\u001b[39m     }\n\u001b[32m     32\u001b[39m     torch.save(save_obj, os.path.join(args.output_dir, \u001b[33m'\u001b[39m\u001b[33mcheckpoint_\u001b[39m\u001b[38;5;132;01m%02d\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m%epoch))\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbarrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# clean up\u001b[39;00m\n\u001b[32m     36\u001b[39m clear_gpu_memory()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/albef/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:83\u001b[39m, in \u001b[36m_exception_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m     85\u001b[39m         msg_dict = _get_msg_dict(func.\u001b[34m__name__\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/albef/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4149\u001b[39m, in \u001b[36mbarrier\u001b[39m\u001b[34m(group, async_op, device_ids)\u001b[39m\n\u001b[32m   4146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   4148\u001b[39m opts = BarrierOptions()\n\u001b[32m-> \u001b[39m\u001b[32m4149\u001b[39m opts.device = \u001b[43m_get_pg_default_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4151\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_ids, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/albef/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:771\u001b[39m, in \u001b[36m_get_pg_default_device\u001b[39m\u001b[34m(group)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_pg_default_device\u001b[39m(group: Optional[ProcessGroup] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.device:\n\u001b[32m    753\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    754\u001b[39m \u001b[33;03m    Return the device to use with ``group`` for control flow usage (object collectives, barrier).\u001b[39;00m\n\u001b[32m    755\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    769\u001b[39m \n\u001b[32m    770\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m     group = group \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m _world.pg_default_device:\n\u001b[32m    773\u001b[39m         \u001b[38;5;66;03m# Previously searched and cached; just return\u001b[39;00m\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _world.pg_default_device[group]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/albef/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:1150\u001b[39m, in \u001b[36m_get_default_group\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1151\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDefault process group has not been initialized, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1152\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease make sure to call init_process_group.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1153\u001b[39m     )\n\u001b[32m   1154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m   1155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember.WORLD)\n",
      "\u001b[31mValueError\u001b[39m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "# training loop, single GPU\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, max_epoch):\n",
    "    if epoch>0:\n",
    "        lr_scheduler.step(epoch+warmup_steps)\n",
    "\n",
    "    if not args.evaluate:\n",
    "        if args.distributed:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_stats = train(model, train_loader, optimizer, tokenizer, epoch, warmup_steps, device, lr_scheduler, config)\n",
    "\n",
    "    if args.evaluate:\n",
    "        break\n",
    "\n",
    "    if utils.is_main_process():\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                      'epoch': epoch,\n",
    "                    }\n",
    "        with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "        save_obj = {\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'config': config,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_%02d.pth'%epoch))\n",
    "    if args.distributed:\n",
    "        dist.barrier()\n",
    "    else:\n",
    "        pass  # Skip barrier for non-distributed training\n",
    "    \n",
    "    # clean up\n",
    "    clear_gpu_memory()\n",
    "\n",
    "vqa_result = evaluation(model, test_loader, tokenizer, device, config)\n",
    "result_file = save_result(vqa_result, args.result_dir, 'vqa_result_epoch%d'%epoch)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "albef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
